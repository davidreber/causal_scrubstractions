\pdfoutput=1
\documentclass{article}


% alptex + small aesthetic tweaks + some extra math defs
\input{preamble/preamble.tex}
\input{preamble/preamble_math}
\input{preamble/definitions_basic}

% bibtex import + some code to strip away useless bib info (volume number, isbn, and the ilk), and to standardize capitalization
% warning: the arxiv uses an outdated bibtex, which causes cryptic and frustrating upload errors 
% easiest solution: install whatever current arxiv texlive is from ftp://tug.org/historic/systems/texlive/ (download the ISO) and compile using this versions pdflatex and bibtex
% alternatively, look upon https://github.com/plk/biblatex/wiki/biblatex-and-the-arXiv and despair. 
\input{preamble/minimalist_biblatex}

\addbibresource{bibs/applications.bib}
\addbibresource{bibs/causal_abstractions.bib}
\addbibresource{bibs/causal_interp.bib}
\addbibresource{bibs/reviews.bib}
\addbibresource{bibs/misc.bib}

\crefformat{equation}{(#2#1#3)}
\crefformat{figure}{Figure~#2#1#3}
\crefname{example}{Example}{Examples}
\crefname{lemma}{Lemma}{Lemmas}
\crefname{cor}{Corollary}{Corollaries}
\crefname{theorem}{Theorem}{Theorems}
\crefname{assumption}{Assumption}{Assumptions}

%********************************************************************
% Extra definitions
%********************************************************************
\usepackage{enumitem} % tight enumerates
\usepackage[separate-uncertainty=true,multi-part-units=single]{siunitx} % better table control

\newcommand{\maxf}[1]{{\cellcolor[gray]{0.8}} #1}
\global\long\def\embedding{\lambda}

% Peter's grey box
\declaretheoremstyle[
%    postheadspace=\newline,
spacebelow=\parsep,
    spaceabove=\parsep,
  mdframed={
    backgroundcolor=gray!10!white,     % vv: weird spacing issue, so leaving transpartent for now
    hidealllines=true, 
    innertopmargin=8pt, 
    innerbottommargin=4pt, 
    skipabove=8pt,
    skipbelow=10pt,
    nobreak=true
}
]{grayboxed}
\declaretheorem[style=grayboxed,name=Assumption]{gassumption}
% \declaretheorem[style=plain]{auxtheorem}
% \declaretheorem[style=grayboxed,sibling=auxtheorem]{algorithm}
% \declaretheorem[style=grayboxed,name=Algorithm]{nalgorithm}
\crefname{gassumption}{Assumption}{Assumptions}

\usepackage{thm-restate}

%********************************************************************
% Dan Roy's commenting code
%********************************************************************
\usepackage{xcolor}
\input{preamble/commenting.tex}
%\input{preamble/myvruler.tex}
%For submission, uncomment these lines to make all annotations render as blank.
% \renewcommand{\LATER}[1]{}
% \renewcommand{\fLATER}[1]{}
% \renewcommand{\TBD}[1]{}
% \renewcommand{\fTBD}[1]{}
% \renewcommand{\PROBLEM}[1]{}
% \renewcommand{\fPROBLEM}[1]{}
% \renewcommand{\NA}[1]{#1}  %% Note, NA's pass through!

% frontmatter
\usepackage[affil-it]{authblk}

\title{Extending Causal Tracing to (hopefully) Validate the Localization Hypothesis}
\date{}
\author{David Reber}
\affil{Computer Science Department, University of Chicago}

\begin{document}
\maketitle

% \begin{abstract}
%   This project introduces an extension to causal scrubbing with the aim of validating the localization hypothesis in the context of Rank-One Model Editing (ROME) in language models. While previous research has critiqued the efficacy of causal tracing for localizing effective model editing points, this work shifts the focus towards enhancing causal scrubbing methods. By explicitly flipping the input instead of employing noise, the revised approach seeks to address the limitations of traditional causal scrubbing. Theoretical analysis and empirical testing are conducted to explore whether this modification can substantiate the localization hypothesis, offering a novel perspective in the field of mechanistic interpretability.
% \end{abstract}

% \section{Preface}
% I was approved to work on the causal scrubbing / ROME paper \cite{meng2022locating} for my final project directly by David McAllister. The paper has similarly accessibile resources as the other 6 assigned papers, including:

% \begin{itemize}
%     \item The original paper\footnote{\url{https://arxiv.org/pdf/2202.05262.pdf}} and supporting website\footnote{\url{https://rome.baulab.info/}}
%     \item Official Pytorch code: \footnote{\url{https://github.com/kmeng01/rome}} and Colab demos (causal tracing\footnote{\url{https://colab.research.google.com/github/kmeng01/rome/blob/main/notebooks/causal_trace.ipynb}}, model editing\footnote{\url{https://colab.research.google.com/github/kmeng01/rome/blob/main/notebooks/rome.ipynb}})
% \end{itemize}

% It's important to note that this project primarily applies to autoregressive models, and so won't use e.g. CIFAR-10. However, a suitable dataset for this project is available, with which GPT-2 XL can be run on Google Colab, making it accessible for interpretability research without extensive data or compute resources.

\section{Introduction}

In ``Locating and Editing Factual Associations in GPT'' by \citet{meng2022locating}, the authors employed \emph{causal tracing} to localize the storage and recall of factual associations in hidden-state activations of certain layers of GPT-2 XL, and then edited these facts via the MLP weights using \emph{Rank-One Model Editing} (ROME). However, causal tracing recent came under scrutiny by ``Does Localization infrom Editing? Surprising Differences in Causality-based Localization vs. Knowledge Editing in Language Models'' \cite{hase2023does}, which contends that localization conclusions from causal tracing do not inform which model MLP layer is best to edit using ROME, challenging the \emph{localization hypothesis} and suggesting that increased mechanistic understanding does not necessarily facilitate effective model steering. 

\begin{definition} {\textbf{Localization Hypothesis}}
  If a ROME edit at a single MLP at layer $i$ and token $j$ is sufficient to restore the uncorrupted label, then intervening on the post-MLP activations at layer $i$ and token $j$ should also flip the label.
\end{definition}

\emph{Model steering} asks `how can we change the weights of the network to produce a desired behavior'? If true, the localization hypothesis indicates that we can intervene on activations first (even though these are inherently context-dependent), to narrow down (`localize') where we should perform interventions on the MLP weights.

\section{Project Proposal}

Based on my recent research\footnote{My prior research 1. formalizes the goal of mechantistic interpretability using the language of causality, 2. establishes a taxonomy for how various interpretability methods (including causal tracing) are only addressing portions of mechanistic interpretability, and 3. establishes partial evaluations for these respective methods. \textbf{Notably, my past work was only theoretical, so all of the empirical investigations of this project will be new contributions.}} into the challenge of obtaining a full mechanistic understanding \cite{reber2023whatsyourusecase}, I conjecture that the failure of causal tracing to localize the best ROME edits is not due to a failure of the localization hypothesis itself, but rather to the inadequacy of causal tracing in providing the promised mechanistic understanding. 

To support this claim, I will reimplement the evaluations of \cite{hase2023does} with a slight modification to causal tracing, which corrupts the inputs not with noise, but rather with another prompt specifically chosen to flip the label. This extension of causal tracing is inspired by the \emph{interchange interventions} of \cite{geiger2023finding,wu2023interpretability}.

%\newpage
\printbibliography

\newpage
\appendix

% \title{Appendix}
% \date{\vspace{-5ex}}
% \maketitle

% \section{Proofs}
% Recall that \cref{thm:mooncracker} is provided to illustrate the use of restatable.
% \mooncracker* 
% \begin{proof}
%   This is straightforward from well-known results of Wallace and Gromit. \PROBLEM{missing citation}
% \end{proof}

\end{document}
